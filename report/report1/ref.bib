
@article{hafner2011reinforcement,
  title={Reinforcement learning in feedback control},
  author={Hafner, Roland and Riedmiller, Martin},
  journal={Machine learning},
  volume={84},
  number={1-2},
  pages={137--169},
  year={2011},
  publisher={Springer}
}

@article{gu2016q,
  title={Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}

@article{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I and Abbeel, Pieter},
  journal={CoRR, abs/1502.05477},
  year={2015}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{paisley2012variational,
  title={Variational Bayesian inference with stochastic search},
  author={Paisley, John and Blei, David and Jordan, Michael},
  journal={arXiv preprint arXiv:1206.6430},
  year={2012}
}

@article{ross2006simulation,
  title={Simulation},
  author={Ross, Sheldon M},
  journal={Burlington, MA: Elsevier},
  year={2006}
}

@inproceedings{peters2006policy,
  title={Policy gradient methods for robotics},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={2219--2225},
  year={2006},
  organization={IEEE}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@incollection{kober2012reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Peters, Jan},
  booktitle={Reinforcement Learning},
  pages={579--610},
  year={2012},
  publisher={Springer}
}

@article{moore1993prioritized,
  title={Prioritized sweeping: Reinforcement learning with less data and less time},
  author={Moore, Andrew W and Atkeson, Christopher G},
  journal={Machine Learning},
  volume={13},
  number={1},
  pages={103--130},
  year={1993},
  publisher={Springer}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{uhlenbeck1930theory,
  title={On the theory of the Brownian motion},
  author={Uhlenbeck, George E and Ornstein, Leonard S},
  journal={Physical review},
  volume={36},
  number={5},
  pages={823},
  year={1930},
  publisher={APS}
}

@inproceedings{silver2014deterministic,
  title={Deterministic Policy Gradient Algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={387--395},
  year={2014}
}

@techreport{lin1993reinforcement,
  title={Reinforcement learning for robots using neural networks},
  author={Lin, Long-Ji},
  year={1993},
  institution={DTIC Document}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={University of Cambridge England}
}

@article{bellemare2012arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  year={2012}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{van2016deep,
  title={Deep Reinforcement Learning with Double Q-Learning},
  author={van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}

@article{rusu2015policy,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}

@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{rajendrana2t,
  title={A2T: Attend, Adapt and Transfer Attentive Deep Architecture for Adaptive Transfer from multiple sources},
  author={Rajendran, Janarthanan and Lakshminarayanan, Aravind and Khapra, Mitesh M and Prasanna, P and Ravindran, Balaraman}
}

@article{stadie2015incentivizing,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}

@inproceedings{thrun1993issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
  year={1993}
}

@article{goodfellow2013empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@inproceedings{sutton1990integrated,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Proceedings of the seventh international conference on machine learning},
  pages={216--224},
  year={1990}
}

@inproceedings{levine2014learning,
  title={Learning neural network policies with guided policy search under unknown dynamics},
  author={Levine, Sergey and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1071--1079},
  year={2014}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{li2004iterative,
  title={Iterative linear quadratic regulator design for nonlinear biological movement systems.},
  author={Li, Weiwei and Todorov, Emanuel},
  booktitle={ICINCO (1)},
  pages={222--229},
  year={2004}
}

@article{levine2013exploring,
  title={Exploring deep and recurrent architectures for optimal control},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1311.1761},
  year={2013}
}

@article{nair2015massively,
  title={Massively parallel methods for deep reinforcement learning},
  author={Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and others},
  journal={arXiv preprint arXiv:1507.04296},
  year={2015}
}

@article{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1602.01783},
  year={2016}
}

@article{tsitsiklis1997analysis,
  title={An analysis of temporal-difference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  journal={IEEE transactions on automatic control},
  volume={42},
  number={5},
  pages={674--690},
  year={1997},
  publisher={IEEE}
}

@inproceedings{schaul2016prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{de2015importance,
  title={The importance of experience replay database composition in deep reinforcement learning},
  author={de Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu{\v{s}}ka, Robert},
  booktitle={Deep Reinforcement Learning Workshop, NIPS},
  year={2015}
}

@article{osband2016deep,
  title={Deep Exploration via Bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1602.04621},
  year={2016}
}

@article{sorokin2015deep,
  title={Deep Attention Recurrent Q-Network},
  author={Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
  journal={arXiv preprint arXiv:1512.01693},
  year={2015}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{thompson1933likelihood,
  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author={Thompson, William R},
  journal={Biometrika},
  volume={25},
  number={3/4},
  pages={285--294},
  year={1933},
  publisher={JSTOR}
}

@book{efron1994introduction,
  title={An introduction to the bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J},
  year={1994},
  publisher={CRC press}
}

@article{hausknecht2015partially,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  journal={arXiv preprint arXiv:1507.06527},
  year={2015}
}

@article{zhang2015towards,
  title={Towards vision-based deep reinforcement learning for robotic motion control},
  author={Zhang, Fangyi and Leitner, J{\"u}rgen and Milford, Michael and Upcroft, Ben and Corke, Peter},
  journal={arXiv preprint arXiv:1511.03791},
  year={2015}
}

@article{lei2016robot,
  title={A robot exploration strategy based on q-learning network},
  author={Lei, Tai and Ming, Liu},
  year={2016}
}

@article{tai2016towards,
  title={Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots},
  author={Lei, Tai and Ming, Lui},
  journal={arXiv preprint arXiv:1610.01733},
  year={2016}
}

@article{narasimhan2015language,
  title={Language understanding for text-based games using deep reinforcement learning},
  author={Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
  journal={arXiv preprint arXiv:1506.08941},
  year={2015}
}

@article{he2015deep,
  title={Deep Reinforcement Learning with an Action Space Defined by Natural Language},
  author={He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
  journal={arXiv preprint arXiv:1511.04636},
  year={2015}
}

@article{guo2015generating,
  title={Generating Text with Deep Reinforcement Learning},
  author={Guo, Hongyu},
  journal={arXiv preprint arXiv:1510.09202},
  year={2015}
}

@article{cuayahuitl2015strategic,
  title={Strategic dialogue management via deep reinforcement learning},
  author={Cuay{\'a}huitl, Heriberto and Keizer, Simon and Lemon, Oliver},
  journal={arXiv preprint arXiv:1511.08099},
  year={2015}
}

@article{levine2016learning,
  title={Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  journal={arXiv preprint arXiv:1603.02199},
  year={2016}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{gu2016continuous,
  title={Continuous Deep Q-Learning with Model-based Acceleration},
  author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:1603.00748},
  year={2016}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={39},
  pages={1--40},
  year={2016}
}

@inproceedings{todorov2005generalized,
  title={A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems},
  author={Todorov, Emanuel and Li, Weiwei},
  booktitle={Proceedings of the 2005, American Control Conference, 2005.},
  pages={300--306},
  year={2005},
  organization={IEEE}
}

@article{hausknecht2015parameterized,
  title={Deep Reinforcement Learning in Parameterized Action Space},
  author={Hausknecht, Matthew and Stone, Peter},
  journal={arXiv preprint arXiv:1511.04143},
  year={2015}
}

@article{gu2016deep,
  title={Deep Reinforcement Learning for Robotic Manipulation},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00633},
  year={2016}
}

@article{gu2016offpolicy,
  title={Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00633},
  year={2016}
}

@article{heess2015memory,
  title={Memory-based control with recurrent neural networks},
  author={Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
  journal={arXiv preprint arXiv:1512.04455},
  year={2015}
}

